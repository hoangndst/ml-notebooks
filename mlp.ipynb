{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLXYemTucHjXN4jOYgSiMG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoangndst/ml-notebooks/blob/master/mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backpropagation\n",
        "- Phương pháp phổ biến nhất để tối ưu MLP vẫn là Gradient Descent (GD). Để áp dụng GD, chúng ta cần tính được gradient của hàm mất mát theo từng ma trận trọng số $\\mathbf{W}^{(l)}$ và và vector bias $\\mathbf{b}^{(l)}$ \n",
        "\n",
        "\\begin{eqnarray}\n",
        "\\mathbf{a}^{(0)} &=& \\mathbf{x} \\newline\n",
        "z_{i}^{(l)} &=& \\mathbf{w}_i^{(l)T}\\mathbf{a}^{(l-1)} + b_i^{(l)} \\newline\n",
        "\\mathbf{z}^{(l)}  &=& \\mathbf{W}^{(l)T}\\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)},~~ l =  1, 2, \\dots, L \\newline\n",
        "\\mathbf{a}^{(l)} &=& f(\\mathbf{z}^{(l)}), ~~ l =  1, 2, \\dots, L \\newline\n",
        "\\mathbf{\\hat{y}} &=& \\mathbf{a}^{(L)}\n",
        "\\end{eqnarray}\n",
        "\n",
        "- Bước này được gói là feedforward vì cách tính toán được thực hiện từ đầu đến cuối của network. MLP cũng được gọi\n",
        "- Giả sử $J(\\mathbf{W, b, X, Y})$ là hàm mất mát.\n",
        "    - Đạo hàm của hàm mất mát theo chỉ một thành phần của ma trận trọng số của lớp cuối cùng:\n",
        "    \\begin{eqnarray}\n",
        "    \\frac{\\partial J}{\\partial w_{ij}^{(L)}} &=& \\frac{\\partial J}{\\partial z_j^{(L)}}. \\frac{\\partial z_j^{(L)}}{\\partial w_{ij}^{(L)}} \\newline\n",
        "    &=& e_j^{(L)} a_i^{(L-1)}\n",
        "    \\end{eqnarray}\n",
        "    - Trong đó $e_j^{(L)} = \\frac{\\partial J}{\\partial z_j^{(L)}}$ là một đại lượng dễ tính. $\\frac{\\partial z_j^{(L)}}{\\partial w_{ij}^{(L)}}  = a_i^{(L-1)}$ là vì $z_j^{(L)} = \\mathbf{w}_j^{(L)T}\\mathbf{a}^{(L-1)} + b_j^{(L)}$\n",
        "    - Tương tự với đạo hàm mất mát theo bias của layer cuối cùng:\n",
        "    \\begin{eqnarray}\n",
        "        \\frac{\\partial J}{\\partial b_{j}^{(L)}} = \\frac{\\partial J}{\\partial z_j^{(L)}}. \\frac{\\partial z_j^{(L)}}{\\partial b_{j}^{(L)}} = e_j^{(L)}\n",
        "    \\end{eqnarray}\n",
        "\n",
        "<img src='https://machinelearningcoban.com/assets/14_mlp/backpropagation.png' width='700px' />\n",
        "\n",
        "- Dựa vào hính trên, ta có thể tính được:\n",
        "\n",
        "    \\begin{eqnarray}\n",
        "\\frac{\\partial J}{\\partial w_{ij}^{(l)}} &=& \\frac{\\partial J}{\\partial z_j^{(l)}}. \\frac{\\partial z_j^{(l)}}{\\partial w_{ij}^{(l)}} \\newline\n",
        "&=& e_j^{(l)} a_i^{(l-1)}\n",
        "\\end{eqnarray}\n",
        "\n",
        "với:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "e_j^{(l)} &=& \\frac{\\partial J}{\\partial z_j^{(l)}} = \\frac{\\partial J}{\\partial a_j^{(l)}} . \\frac{\\partial a_j^{(l)}}{\\partial z_j^{(l)}} \\newline\n",
        "&=& \\left( \\sum_{k = 1}^{d^{(l+1)}} \\frac{\\partial J}{\\partial z_k^{(l+1)}} .\\frac{\\partial z_k^{(l+1)}}{\\partial a_j^{(l)}} \\right) f’(z_j^{(l)}) \\newline\n",
        " &=&\\left( \\sum_{k = 1}^{d^{(l+1)}} e_k^{(l+1)} w_{jk}^{(l+1)} \\right) f’(z_j^{(l)}) \\newline\n",
        " &=&\\left( \\mathbf{w}_{j:}^{(l+1)} \\mathbf{e}^{(l+1)} \\right) f’(z_j^{(l)}) \\newline\n",
        "\\end{eqnarray}\n"
      ],
      "metadata": {
        "id": "J5WLMla8-yOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backpropagation cho Stochastic Gradient Descent\n",
        "- Đạo hàm theo từng hệ số\n",
        "    "
      ],
      "metadata": {
        "id": "mD0cglTdCw4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# source: https://machinelearningcoban.com/2017/02/24/mlp/#-gioi-thieu\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "N = 100 # number of points per class\n",
        "d0 = 2 # dimensionality\n",
        "C = 3 # number of classes\n",
        "X = np.zeros((d0, N*C)) # data matrix (each row = single example)\n",
        "y = np.zeros(N*C, dtype='uint8') # class labels\n",
        "\n",
        "for j in range(C):\n",
        "  ix = range(N*j,N*(j+1))\n",
        "  r = np.linspace(0.0,1,N) # radius\n",
        "  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
        "  X[:,ix] = np.c_[r*np.sin(t), r*np.cos(t)].T\n",
        "  y[ix] = j"
      ],
      "metadata": {
        "id": "mZ5ut-ghHH3p"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_data():\n",
        "    plt.plot(X[0, :N], X[1, :N], 'bs', markersize = 7);\n",
        "    plt.plot(X[0, N:2*N], X[1, N:2*N], 'ro', markersize = 7);\n",
        "    plt.plot(X[0, 2*N:], X[1, 2*N:], 'g^', markersize = 7);\n",
        "    # plt.axis('off')\n",
        "    plt.xlim([-1.5, 1.5])\n",
        "    plt.ylim([-1.5, 1.5])\n",
        "    cur_axes = plt.gca()\n",
        "    cur_axes.axes.get_xaxis().set_ticks([])\n",
        "    cur_axes.axes.get_yaxis().set_ticks([])\n",
        "\n",
        "    # plt.savefig('EX.png', bbox_inches='tight', dpi = 600)\n",
        "    plt.show()\n",
        "display_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "fbVxaID2eKvB",
        "outputId": "224d8c36-7f66-4cda-d004-2b769564ce1f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mô hình trainning\n",
        "<img src='https://machinelearningcoban.com/assets/14_mlp/ex_nn.png' witdh='700px' />\n",
        "\n",
        "### Tính toán feedforward\n",
        "\n",
        "\\begin{eqnarray}\n",
        "\\mathbf{Z}^{(1)} &=& \\mathbf{W}^{(1)T}\\mathbf{X} \\newline\n",
        "\\mathbf{A}^{(1)} &=& \\max(\\mathbf{Z}^{(1)}, \\mathbf{0}) \\newline\n",
        "\\mathbf{Z}^{(2)} &=& \\mathbf{W}^{(2)T}\\mathbf{A}^{(1)} \\newline\n",
        "\\mathbf{\\hat{Y}} = \\mathbf{A}^{(2)} &=& \\text{softmax}(\\mathbf{Z}^{(2)})\n",
        "\\end{eqnarray}\n",
        "\n",
        "- Hàm mất mát được tính như sau:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "J \\triangleq J(\\mathbf{W, b}; \\mathbf{X, Y}) = -\\frac{1}{N}\\sum_{i = 1}^N \\sum_{j = 1}^C y_{ji}\\log(\\hat{y}_{ji})\n",
        "\\end{eqnarray}\n",
        "\n",
        "### Tính toán Backpropagation\n",
        "\n",
        "\\begin{eqnarray}\n",
        "\\mathbf{E}^{(2)} &=& \\frac{\\partial J}{\\partial \\mathbf{Z}^{(2)}} =\\frac{1}{N}(\\mathbf{\\hat{Y}} - \\mathbf{Y}) \\newline\n",
        "\\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}} &=& \\mathbf{A}^{(1)}  \\mathbf{E}^{(2)T} \\newline\n",
        "\\frac{\\partial J}{\\partial \\mathbf{b}^{(2)}} &=& \\sum_{n=1}^N\\mathbf{e}_n^{(2)} \\newline\n",
        "\\mathbf{E}^{(1)} &=& \\left(\\mathbf{W}^{(2)}\\mathbf{E}^{(2)}\\right) \\odot f’(\\mathbf{Z}^{(1)}) \\newline\n",
        "\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}} &=& \\mathbf{A}^{(0)}  \\mathbf{E}^{(1)T} = \\mathbf{X}\\mathbf{E}^{(1)T}\\newline\n",
        "\\frac{\\partial J}{\\partial \\mathbf{b}^{(1)}} &=& \\sum_{n=1}^N\\mathbf{e}_n^{(1)} \\newline\n",
        "\\end{eqnarray}\n"
      ],
      "metadata": {
        "id": "BQmJaosIH8CM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(V):\n",
        "    e_V = np.exp(V - np.max(V, axis = 0, keepdims = True))\n",
        "    Z = e_V / e_V.sum(axis = 0)\n",
        "    return Z\n",
        "\n",
        "def ReLU(V):\n",
        "    \n",
        "\n",
        "## One-hot coding\n",
        "from scipy import sparse\n",
        "def convert_labels(y, C = 3):\n",
        "    Y = sparse.coo_matrix((np.ones_like(y),\n",
        "        (y, np.arange(len(y)))), shape = (C, len(y))).toarray()\n",
        "    return Y\n",
        "\n",
        "# cost or loss function\n",
        "def cost(Y, Yhat):\n",
        "    return -np.sum(Y*np.log(Yhat))/Y.shape[1]\n",
        "\n",
        "print('X: \\n', X.shape)\n",
        "print('y: \\n', y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPGxr5PlKeQr",
        "outputId": "348d42fb-5006-4252-a051-e1fe6bd03f66"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X: \n",
            " (2, 300)\n",
            "y: \n",
            " (300,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d0 = 2\n",
        "d1 = h = 100 # size of hidden layer\n",
        "d2 = C = 3\n",
        "# initialize parameters randomly\n",
        "W1 = 0.01*np.random.randn(d0, d1)\n",
        "b1 = np.zeros((d1, 1))\n",
        "W2 = 0.01*np.random.randn(d1, d2)\n",
        "b2 = np.zeros((d2, 1))\n",
        "\n",
        "Y = convert_labels(y, C)\n",
        "N = X.shape[1]\n",
        "eta = 1 # learning rate\n",
        "print('W_1: \\n', W1.shape)\n",
        "print('W_2: \\n', W2.shape)\n",
        "print('X: \\n', X.shape)\n",
        "print('y: \\n', Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzAqEARrLBW2",
        "outputId": "dcebe585-4620-411f-e2ab-8e59c9363e4f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W_1: \n",
            " (2, 100)\n",
            "W_2: \n",
            " (100, 3)\n",
            "X: \n",
            " (2, 300)\n",
            "y: \n",
            " [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1000000):\n",
        "    # Feedforward\n",
        "    Z1 = np.dot(W1.T, X) + b1\n",
        "    A1 = np.maximum(Z1, 0)\n",
        "    Z2 = np.dot(W2.T, A1) + b2\n",
        "    Y_hat = softmax(Z2)\n",
        "\n",
        "    # print loss after each 1000 iterations\n",
        "    if i %1000 == 0:\n",
        "        # compute the loss: average cross-entropy loss\n",
        "        loss = cost(Y, Y_hat)\n",
        "        print(\"iter %d, loss: %f\" %(i, loss))\n",
        "    # backpropagation\n",
        "    E2 = (Y_hat - Y) / N\n",
        "    dW2 = np.dot(A1, E2.T)\n",
        "    db2 = np.sum(E2, axis = 1, keepdims = True)\n",
        "    E1 = np.dot(W2, E2)\n",
        "    E1[Z1 <= 0] = 0 # gradient ReLU\n",
        "    dW1 = np.dot(X, E1.T)\n",
        "    db1 = np.sum(E1, axis = 1, keepdims = True)\n",
        "\n",
        "    # Update GD\n",
        "    W1 += -eta*dW1\n",
        "    b1 += -eta*db1\n",
        "    W2 += -eta*dW2\n",
        "    b2 += -eta*db2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHSZQhqGMALz",
        "outputId": "46dcabb8-d014-4931-82f2-e66606a6760b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0, loss: 1.098545\n",
            "iter 1000, loss: 0.158888\n",
            "iter 2000, loss: 0.051854\n",
            "iter 3000, loss: 0.035895\n",
            "iter 4000, loss: 0.029127\n",
            "iter 5000, loss: 0.025598\n",
            "iter 6000, loss: 0.023390\n",
            "iter 7000, loss: 0.021846\n",
            "iter 8000, loss: 0.020674\n",
            "iter 9000, loss: 0.019747\n",
            "iter 10000, loss: 0.018991\n",
            "iter 11000, loss: 0.018375\n",
            "iter 12000, loss: 0.017845\n",
            "iter 13000, loss: 0.017371\n",
            "iter 14000, loss: 0.016962\n",
            "iter 15000, loss: 0.016585\n",
            "iter 16000, loss: 0.016252\n",
            "iter 17000, loss: 0.015965\n",
            "iter 18000, loss: 0.015711\n",
            "iter 19000, loss: 0.015483\n",
            "iter 20000, loss: 0.015276\n",
            "iter 21000, loss: 0.015084\n",
            "iter 22000, loss: 0.014909\n",
            "iter 23000, loss: 0.014748\n",
            "iter 24000, loss: 0.014588\n",
            "iter 25000, loss: 0.014440\n",
            "iter 26000, loss: 0.014304\n",
            "iter 27000, loss: 0.014178\n",
            "iter 28000, loss: 0.014062\n",
            "iter 29000, loss: 0.013955\n",
            "iter 30000, loss: 0.013855\n",
            "iter 31000, loss: 0.013762\n",
            "iter 32000, loss: 0.013674\n",
            "iter 33000, loss: 0.013591\n",
            "iter 34000, loss: 0.013513\n",
            "iter 35000, loss: 0.013438\n",
            "iter 36000, loss: 0.013368\n",
            "iter 37000, loss: 0.013302\n",
            "iter 38000, loss: 0.013238\n",
            "iter 39000, loss: 0.013178\n",
            "iter 40000, loss: 0.013121\n",
            "iter 41000, loss: 0.013065\n",
            "iter 42000, loss: 0.013012\n",
            "iter 43000, loss: 0.012961\n",
            "iter 44000, loss: 0.012912\n",
            "iter 45000, loss: 0.012865\n",
            "iter 46000, loss: 0.012820\n",
            "iter 47000, loss: 0.012777\n",
            "iter 48000, loss: 0.012735\n",
            "iter 49000, loss: 0.012695\n",
            "iter 50000, loss: 0.012656\n",
            "iter 51000, loss: 0.012619\n",
            "iter 52000, loss: 0.012583\n",
            "iter 53000, loss: 0.012547\n",
            "iter 54000, loss: 0.012513\n",
            "iter 55000, loss: 0.012481\n",
            "iter 56000, loss: 0.012450\n",
            "iter 57000, loss: 0.012421\n",
            "iter 58000, loss: 0.012392\n",
            "iter 59000, loss: 0.012364\n",
            "iter 60000, loss: 0.012338\n",
            "iter 61000, loss: 0.012312\n",
            "iter 62000, loss: 0.012287\n",
            "iter 63000, loss: 0.012262\n",
            "iter 64000, loss: 0.012239\n",
            "iter 65000, loss: 0.012216\n",
            "iter 66000, loss: 0.012194\n",
            "iter 67000, loss: 0.012172\n",
            "iter 68000, loss: 0.012152\n",
            "iter 69000, loss: 0.012131\n",
            "iter 70000, loss: 0.012112\n",
            "iter 71000, loss: 0.012093\n",
            "iter 72000, loss: 0.012075\n",
            "iter 73000, loss: 0.012057\n",
            "iter 74000, loss: 0.012039\n",
            "iter 75000, loss: 0.012023\n",
            "iter 76000, loss: 0.012006\n",
            "iter 77000, loss: 0.011990\n",
            "iter 78000, loss: 0.011975\n",
            "iter 79000, loss: 0.011960\n",
            "iter 80000, loss: 0.011945\n",
            "iter 81000, loss: 0.011931\n",
            "iter 82000, loss: 0.011917\n",
            "iter 83000, loss: 0.011903\n",
            "iter 84000, loss: 0.011890\n",
            "iter 85000, loss: 0.011877\n",
            "iter 86000, loss: 0.011865\n",
            "iter 87000, loss: 0.011852\n",
            "iter 88000, loss: 0.011840\n",
            "iter 89000, loss: 0.011829\n",
            "iter 90000, loss: 0.011817\n",
            "iter 91000, loss: 0.011806\n",
            "iter 92000, loss: 0.011795\n",
            "iter 93000, loss: 0.011784\n",
            "iter 94000, loss: 0.011774\n",
            "iter 95000, loss: 0.011763\n",
            "iter 96000, loss: 0.011753\n",
            "iter 97000, loss: 0.011744\n",
            "iter 98000, loss: 0.011734\n",
            "iter 99000, loss: 0.011724\n",
            "iter 100000, loss: 0.011715\n",
            "iter 101000, loss: 0.011706\n",
            "iter 102000, loss: 0.011697\n",
            "iter 103000, loss: 0.011689\n",
            "iter 104000, loss: 0.011680\n",
            "iter 105000, loss: 0.011672\n",
            "iter 106000, loss: 0.011664\n",
            "iter 107000, loss: 0.011656\n",
            "iter 108000, loss: 0.011648\n",
            "iter 109000, loss: 0.011640\n",
            "iter 110000, loss: 0.011633\n",
            "iter 111000, loss: 0.011626\n",
            "iter 112000, loss: 0.011619\n",
            "iter 113000, loss: 0.011611\n",
            "iter 114000, loss: 0.011605\n",
            "iter 115000, loss: 0.011598\n",
            "iter 116000, loss: 0.011591\n",
            "iter 117000, loss: 0.011585\n",
            "iter 118000, loss: 0.011578\n",
            "iter 119000, loss: 0.011572\n",
            "iter 120000, loss: 0.011566\n",
            "iter 121000, loss: 0.011560\n",
            "iter 122000, loss: 0.011554\n",
            "iter 123000, loss: 0.011548\n",
            "iter 124000, loss: 0.011542\n",
            "iter 125000, loss: 0.011536\n",
            "iter 126000, loss: 0.011531\n",
            "iter 127000, loss: 0.011525\n",
            "iter 128000, loss: 0.011520\n",
            "iter 129000, loss: 0.011515\n",
            "iter 130000, loss: 0.011510\n",
            "iter 131000, loss: 0.011504\n",
            "iter 132000, loss: 0.011499\n",
            "iter 133000, loss: 0.011495\n",
            "iter 134000, loss: 0.011490\n",
            "iter 135000, loss: 0.011485\n",
            "iter 136000, loss: 0.011480\n",
            "iter 137000, loss: 0.011476\n",
            "iter 138000, loss: 0.011471\n",
            "iter 139000, loss: 0.011467\n",
            "iter 140000, loss: 0.011462\n",
            "iter 141000, loss: 0.011458\n",
            "iter 142000, loss: 0.011453\n",
            "iter 143000, loss: 0.011449\n",
            "iter 144000, loss: 0.011445\n",
            "iter 145000, loss: 0.011441\n",
            "iter 146000, loss: 0.011437\n",
            "iter 147000, loss: 0.011433\n",
            "iter 148000, loss: 0.011429\n",
            "iter 149000, loss: 0.011425\n",
            "iter 150000, loss: 0.011421\n",
            "iter 151000, loss: 0.011418\n",
            "iter 152000, loss: 0.011414\n",
            "iter 153000, loss: 0.011410\n",
            "iter 154000, loss: 0.011407\n",
            "iter 155000, loss: 0.011403\n",
            "iter 156000, loss: 0.011400\n",
            "iter 157000, loss: 0.011396\n",
            "iter 158000, loss: 0.011393\n",
            "iter 159000, loss: 0.011389\n",
            "iter 160000, loss: 0.011386\n",
            "iter 161000, loss: 0.011383\n",
            "iter 162000, loss: 0.011380\n",
            "iter 163000, loss: 0.011377\n",
            "iter 164000, loss: 0.011374\n",
            "iter 165000, loss: 0.011370\n",
            "iter 166000, loss: 0.011367\n",
            "iter 167000, loss: 0.011364\n",
            "iter 168000, loss: 0.011362\n",
            "iter 169000, loss: 0.011359\n",
            "iter 170000, loss: 0.011356\n",
            "iter 171000, loss: 0.011353\n",
            "iter 172000, loss: 0.011350\n",
            "iter 173000, loss: 0.011347\n",
            "iter 174000, loss: 0.011345\n",
            "iter 175000, loss: 0.011342\n",
            "iter 176000, loss: 0.011339\n",
            "iter 177000, loss: 0.011337\n",
            "iter 178000, loss: 0.011334\n",
            "iter 179000, loss: 0.011332\n",
            "iter 180000, loss: 0.011329\n",
            "iter 181000, loss: 0.011327\n",
            "iter 182000, loss: 0.011324\n",
            "iter 183000, loss: 0.011322\n",
            "iter 184000, loss: 0.011320\n",
            "iter 185000, loss: 0.011317\n",
            "iter 186000, loss: 0.011315\n",
            "iter 187000, loss: 0.011313\n",
            "iter 188000, loss: 0.011310\n",
            "iter 189000, loss: 0.011308\n",
            "iter 190000, loss: 0.011306\n",
            "iter 191000, loss: 0.011304\n",
            "iter 192000, loss: 0.011302\n",
            "iter 193000, loss: 0.011299\n",
            "iter 194000, loss: 0.011297\n",
            "iter 195000, loss: 0.011295\n",
            "iter 196000, loss: 0.011293\n",
            "iter 197000, loss: 0.011291\n",
            "iter 198000, loss: 0.011289\n",
            "iter 199000, loss: 0.011287\n",
            "iter 200000, loss: 0.011285\n",
            "iter 201000, loss: 0.011283\n",
            "iter 202000, loss: 0.011281\n",
            "iter 203000, loss: 0.011279\n",
            "iter 204000, loss: 0.011277\n",
            "iter 205000, loss: 0.011276\n",
            "iter 206000, loss: 0.011274\n",
            "iter 207000, loss: 0.011272\n",
            "iter 208000, loss: 0.011270\n",
            "iter 209000, loss: 0.011268\n",
            "iter 210000, loss: 0.011267\n",
            "iter 211000, loss: 0.011265\n",
            "iter 212000, loss: 0.011263\n",
            "iter 213000, loss: 0.011261\n",
            "iter 214000, loss: 0.011260\n",
            "iter 215000, loss: 0.011258\n",
            "iter 216000, loss: 0.011257\n",
            "iter 217000, loss: 0.011255\n",
            "iter 218000, loss: 0.011253\n",
            "iter 219000, loss: 0.011252\n",
            "iter 220000, loss: 0.011250\n",
            "iter 221000, loss: 0.011249\n",
            "iter 222000, loss: 0.011247\n",
            "iter 223000, loss: 0.011246\n",
            "iter 224000, loss: 0.011244\n",
            "iter 225000, loss: 0.011243\n",
            "iter 226000, loss: 0.011241\n",
            "iter 227000, loss: 0.011240\n",
            "iter 228000, loss: 0.011238\n",
            "iter 229000, loss: 0.011237\n",
            "iter 230000, loss: 0.011235\n",
            "iter 231000, loss: 0.011234\n",
            "iter 232000, loss: 0.011233\n",
            "iter 233000, loss: 0.011231\n",
            "iter 234000, loss: 0.011230\n",
            "iter 235000, loss: 0.011228\n",
            "iter 236000, loss: 0.011227\n",
            "iter 237000, loss: 0.011226\n",
            "iter 238000, loss: 0.011224\n",
            "iter 239000, loss: 0.011223\n",
            "iter 240000, loss: 0.011222\n",
            "iter 241000, loss: 0.011221\n",
            "iter 242000, loss: 0.011219\n",
            "iter 243000, loss: 0.011218\n",
            "iter 244000, loss: 0.011217\n",
            "iter 245000, loss: 0.011216\n",
            "iter 246000, loss: 0.011214\n",
            "iter 247000, loss: 0.011213\n",
            "iter 248000, loss: 0.011212\n",
            "iter 249000, loss: 0.011211\n",
            "iter 250000, loss: 0.011210\n",
            "iter 251000, loss: 0.011209\n",
            "iter 252000, loss: 0.011207\n",
            "iter 253000, loss: 0.011206\n",
            "iter 254000, loss: 0.011205\n",
            "iter 255000, loss: 0.011204\n",
            "iter 256000, loss: 0.011203\n",
            "iter 257000, loss: 0.011202\n",
            "iter 258000, loss: 0.011201\n",
            "iter 259000, loss: 0.011200\n",
            "iter 260000, loss: 0.011199\n",
            "iter 261000, loss: 0.011198\n",
            "iter 262000, loss: 0.011197\n",
            "iter 263000, loss: 0.011196\n",
            "iter 264000, loss: 0.011195\n",
            "iter 265000, loss: 0.011193\n",
            "iter 266000, loss: 0.011192\n",
            "iter 267000, loss: 0.011191\n",
            "iter 268000, loss: 0.011190\n",
            "iter 269000, loss: 0.011190\n",
            "iter 270000, loss: 0.011189\n",
            "iter 271000, loss: 0.011188\n",
            "iter 272000, loss: 0.011187\n",
            "iter 273000, loss: 0.011186\n",
            "iter 274000, loss: 0.011185\n",
            "iter 275000, loss: 0.011184\n",
            "iter 276000, loss: 0.011183\n",
            "iter 277000, loss: 0.011182\n",
            "iter 278000, loss: 0.011181\n",
            "iter 279000, loss: 0.011180\n",
            "iter 280000, loss: 0.011179\n",
            "iter 281000, loss: 0.011178\n",
            "iter 282000, loss: 0.011177\n",
            "iter 283000, loss: 0.011177\n",
            "iter 284000, loss: 0.011176\n",
            "iter 285000, loss: 0.011175\n",
            "iter 286000, loss: 0.011174\n",
            "iter 287000, loss: 0.011173\n",
            "iter 288000, loss: 0.011172\n",
            "iter 289000, loss: 0.011172\n",
            "iter 290000, loss: 0.011171\n",
            "iter 291000, loss: 0.011170\n",
            "iter 292000, loss: 0.011169\n",
            "iter 293000, loss: 0.011168\n",
            "iter 294000, loss: 0.011167\n",
            "iter 295000, loss: 0.011167\n",
            "iter 296000, loss: 0.011166\n",
            "iter 297000, loss: 0.011165\n",
            "iter 298000, loss: 0.011164\n",
            "iter 299000, loss: 0.011164\n",
            "iter 300000, loss: 0.011163\n",
            "iter 301000, loss: 0.011162\n",
            "iter 302000, loss: 0.011161\n",
            "iter 303000, loss: 0.011161\n",
            "iter 304000, loss: 0.011160\n",
            "iter 305000, loss: 0.011159\n",
            "iter 306000, loss: 0.011158\n",
            "iter 307000, loss: 0.011158\n",
            "iter 308000, loss: 0.011157\n",
            "iter 309000, loss: 0.011156\n",
            "iter 310000, loss: 0.011155\n",
            "iter 311000, loss: 0.011155\n",
            "iter 312000, loss: 0.011154\n",
            "iter 313000, loss: 0.011153\n",
            "iter 314000, loss: 0.011153\n",
            "iter 315000, loss: 0.011152\n",
            "iter 316000, loss: 0.011151\n",
            "iter 317000, loss: 0.011151\n",
            "iter 318000, loss: 0.011150\n",
            "iter 319000, loss: 0.011149\n",
            "iter 320000, loss: 0.011149\n",
            "iter 321000, loss: 0.011148\n",
            "iter 322000, loss: 0.011147\n",
            "iter 323000, loss: 0.011147\n",
            "iter 324000, loss: 0.011146\n",
            "iter 325000, loss: 0.011146\n",
            "iter 326000, loss: 0.011145\n",
            "iter 327000, loss: 0.011144\n",
            "iter 328000, loss: 0.011144\n",
            "iter 329000, loss: 0.011143\n",
            "iter 330000, loss: 0.011142\n",
            "iter 331000, loss: 0.011142\n",
            "iter 332000, loss: 0.011141\n",
            "iter 333000, loss: 0.011141\n",
            "iter 334000, loss: 0.011140\n",
            "iter 335000, loss: 0.011139\n",
            "iter 336000, loss: 0.011139\n",
            "iter 337000, loss: 0.011138\n",
            "iter 338000, loss: 0.011138\n",
            "iter 339000, loss: 0.011137\n",
            "iter 340000, loss: 0.011137\n",
            "iter 341000, loss: 0.011136\n",
            "iter 342000, loss: 0.011135\n",
            "iter 343000, loss: 0.011135\n",
            "iter 344000, loss: 0.011134\n",
            "iter 345000, loss: 0.011134\n",
            "iter 346000, loss: 0.011133\n",
            "iter 347000, loss: 0.011133\n",
            "iter 348000, loss: 0.011132\n",
            "iter 349000, loss: 0.011132\n",
            "iter 350000, loss: 0.011131\n",
            "iter 351000, loss: 0.011131\n",
            "iter 352000, loss: 0.011130\n",
            "iter 353000, loss: 0.011129\n",
            "iter 354000, loss: 0.011129\n",
            "iter 355000, loss: 0.011128\n",
            "iter 356000, loss: 0.011128\n",
            "iter 357000, loss: 0.011127\n",
            "iter 358000, loss: 0.011127\n",
            "iter 359000, loss: 0.011126\n",
            "iter 360000, loss: 0.011126\n",
            "iter 361000, loss: 0.011125\n",
            "iter 362000, loss: 0.011125\n",
            "iter 363000, loss: 0.011124\n",
            "iter 364000, loss: 0.011124\n",
            "iter 365000, loss: 0.011124\n",
            "iter 366000, loss: 0.011123\n",
            "iter 367000, loss: 0.011123\n",
            "iter 368000, loss: 0.011122\n",
            "iter 369000, loss: 0.011122\n",
            "iter 370000, loss: 0.011121\n",
            "iter 371000, loss: 0.011121\n",
            "iter 372000, loss: 0.011120\n",
            "iter 373000, loss: 0.011120\n",
            "iter 374000, loss: 0.011119\n",
            "iter 375000, loss: 0.011119\n",
            "iter 376000, loss: 0.011118\n",
            "iter 377000, loss: 0.011118\n",
            "iter 378000, loss: 0.011118\n",
            "iter 379000, loss: 0.011117\n",
            "iter 380000, loss: 0.011117\n",
            "iter 381000, loss: 0.011116\n",
            "iter 382000, loss: 0.011116\n",
            "iter 383000, loss: 0.011115\n",
            "iter 384000, loss: 0.011115\n",
            "iter 385000, loss: 0.011115\n",
            "iter 386000, loss: 0.011114\n",
            "iter 387000, loss: 0.011114\n",
            "iter 388000, loss: 0.011113\n",
            "iter 389000, loss: 0.011113\n",
            "iter 390000, loss: 0.011112\n",
            "iter 391000, loss: 0.011112\n",
            "iter 392000, loss: 0.011112\n",
            "iter 393000, loss: 0.011111\n",
            "iter 394000, loss: 0.011111\n",
            "iter 395000, loss: 0.011110\n",
            "iter 396000, loss: 0.011110\n",
            "iter 397000, loss: 0.011110\n",
            "iter 398000, loss: 0.011109\n",
            "iter 399000, loss: 0.011109\n",
            "iter 400000, loss: 0.011108\n",
            "iter 401000, loss: 0.011108\n",
            "iter 402000, loss: 0.011108\n",
            "iter 403000, loss: 0.011107\n",
            "iter 404000, loss: 0.011107\n",
            "iter 405000, loss: 0.011107\n",
            "iter 406000, loss: 0.011106\n",
            "iter 407000, loss: 0.011106\n",
            "iter 408000, loss: 0.011105\n",
            "iter 409000, loss: 0.011105\n",
            "iter 410000, loss: 0.011105\n",
            "iter 411000, loss: 0.011104\n",
            "iter 412000, loss: 0.011104\n",
            "iter 413000, loss: 0.011104\n",
            "iter 414000, loss: 0.011103\n",
            "iter 415000, loss: 0.011103\n",
            "iter 416000, loss: 0.011103\n",
            "iter 417000, loss: 0.011102\n",
            "iter 418000, loss: 0.011102\n",
            "iter 419000, loss: 0.011101\n",
            "iter 420000, loss: 0.011101\n",
            "iter 421000, loss: 0.011101\n",
            "iter 422000, loss: 0.011100\n",
            "iter 423000, loss: 0.011100\n",
            "iter 424000, loss: 0.011100\n",
            "iter 425000, loss: 0.011099\n",
            "iter 426000, loss: 0.011099\n",
            "iter 427000, loss: 0.011099\n",
            "iter 428000, loss: 0.011098\n",
            "iter 429000, loss: 0.011098\n",
            "iter 430000, loss: 0.011098\n",
            "iter 431000, loss: 0.011097\n",
            "iter 432000, loss: 0.011097\n",
            "iter 433000, loss: 0.011097\n",
            "iter 434000, loss: 0.011096\n",
            "iter 435000, loss: 0.011096\n",
            "iter 436000, loss: 0.011096\n",
            "iter 437000, loss: 0.011095\n",
            "iter 438000, loss: 0.011095\n",
            "iter 439000, loss: 0.011095\n",
            "iter 440000, loss: 0.011095\n",
            "iter 441000, loss: 0.011094\n",
            "iter 442000, loss: 0.011094\n",
            "iter 443000, loss: 0.011094\n",
            "iter 444000, loss: 0.011093\n",
            "iter 445000, loss: 0.011093\n",
            "iter 446000, loss: 0.011093\n",
            "iter 447000, loss: 0.011092\n",
            "iter 448000, loss: 0.011092\n",
            "iter 449000, loss: 0.011092\n",
            "iter 450000, loss: 0.011092\n",
            "iter 451000, loss: 0.011091\n",
            "iter 452000, loss: 0.011091\n",
            "iter 453000, loss: 0.011091\n",
            "iter 454000, loss: 0.011090\n",
            "iter 455000, loss: 0.011090\n",
            "iter 456000, loss: 0.011090\n",
            "iter 457000, loss: 0.011089\n",
            "iter 458000, loss: 0.011089\n",
            "iter 459000, loss: 0.011089\n",
            "iter 460000, loss: 0.011089\n",
            "iter 461000, loss: 0.011088\n",
            "iter 462000, loss: 0.011088\n",
            "iter 463000, loss: 0.011088\n",
            "iter 464000, loss: 0.011088\n",
            "iter 465000, loss: 0.011087\n",
            "iter 466000, loss: 0.011087\n",
            "iter 467000, loss: 0.011087\n",
            "iter 468000, loss: 0.011086\n",
            "iter 469000, loss: 0.011086\n",
            "iter 470000, loss: 0.011086\n",
            "iter 471000, loss: 0.011086\n",
            "iter 472000, loss: 0.011085\n",
            "iter 473000, loss: 0.011085\n",
            "iter 474000, loss: 0.011085\n",
            "iter 475000, loss: 0.011085\n",
            "iter 476000, loss: 0.011084\n",
            "iter 477000, loss: 0.011084\n",
            "iter 478000, loss: 0.011084\n",
            "iter 479000, loss: 0.011084\n",
            "iter 480000, loss: 0.011083\n",
            "iter 481000, loss: 0.011083\n",
            "iter 482000, loss: 0.011083\n",
            "iter 483000, loss: 0.011083\n",
            "iter 484000, loss: 0.011082\n",
            "iter 485000, loss: 0.011082\n",
            "iter 486000, loss: 0.011082\n",
            "iter 487000, loss: 0.011082\n",
            "iter 488000, loss: 0.011081\n",
            "iter 489000, loss: 0.011081\n",
            "iter 490000, loss: 0.011081\n",
            "iter 491000, loss: 0.011081\n",
            "iter 492000, loss: 0.011080\n",
            "iter 493000, loss: 0.011080\n",
            "iter 494000, loss: 0.011080\n",
            "iter 495000, loss: 0.011080\n",
            "iter 496000, loss: 0.011079\n",
            "iter 497000, loss: 0.011079\n",
            "iter 498000, loss: 0.011079\n",
            "iter 499000, loss: 0.011079\n",
            "iter 500000, loss: 0.011078\n",
            "iter 501000, loss: 0.011078\n",
            "iter 502000, loss: 0.011078\n",
            "iter 503000, loss: 0.011078\n",
            "iter 504000, loss: 0.011078\n",
            "iter 505000, loss: 0.011077\n",
            "iter 506000, loss: 0.011077\n",
            "iter 507000, loss: 0.011077\n",
            "iter 508000, loss: 0.011077\n",
            "iter 509000, loss: 0.011076\n",
            "iter 510000, loss: 0.011076\n",
            "iter 511000, loss: 0.011076\n",
            "iter 512000, loss: 0.011076\n",
            "iter 513000, loss: 0.011076\n",
            "iter 514000, loss: 0.011075\n",
            "iter 515000, loss: 0.011075\n",
            "iter 516000, loss: 0.011075\n",
            "iter 517000, loss: 0.011075\n",
            "iter 518000, loss: 0.011074\n",
            "iter 519000, loss: 0.011074\n",
            "iter 520000, loss: 0.011074\n",
            "iter 521000, loss: 0.011074\n",
            "iter 522000, loss: 0.011074\n",
            "iter 523000, loss: 0.011073\n",
            "iter 524000, loss: 0.011073\n",
            "iter 525000, loss: 0.011073\n",
            "iter 526000, loss: 0.011073\n",
            "iter 527000, loss: 0.011073\n",
            "iter 528000, loss: 0.011072\n",
            "iter 529000, loss: 0.011072\n",
            "iter 530000, loss: 0.011072\n",
            "iter 531000, loss: 0.011072\n",
            "iter 532000, loss: 0.011072\n",
            "iter 533000, loss: 0.011071\n",
            "iter 534000, loss: 0.011071\n",
            "iter 535000, loss: 0.011071\n",
            "iter 536000, loss: 0.011071\n",
            "iter 537000, loss: 0.011071\n",
            "iter 538000, loss: 0.011070\n",
            "iter 539000, loss: 0.011070\n",
            "iter 540000, loss: 0.011070\n",
            "iter 541000, loss: 0.011070\n",
            "iter 542000, loss: 0.011070\n",
            "iter 543000, loss: 0.011069\n",
            "iter 544000, loss: 0.011069\n",
            "iter 545000, loss: 0.011069\n",
            "iter 546000, loss: 0.011069\n",
            "iter 547000, loss: 0.011069\n",
            "iter 548000, loss: 0.011068\n",
            "iter 549000, loss: 0.011068\n",
            "iter 550000, loss: 0.011068\n",
            "iter 551000, loss: 0.011068\n",
            "iter 552000, loss: 0.011068\n",
            "iter 553000, loss: 0.011067\n",
            "iter 554000, loss: 0.011067\n",
            "iter 555000, loss: 0.011067\n",
            "iter 556000, loss: 0.011067\n",
            "iter 557000, loss: 0.011067\n",
            "iter 558000, loss: 0.011067\n",
            "iter 559000, loss: 0.011066\n",
            "iter 560000, loss: 0.011066\n",
            "iter 561000, loss: 0.011066\n",
            "iter 562000, loss: 0.011066\n",
            "iter 563000, loss: 0.011066\n",
            "iter 564000, loss: 0.011066\n",
            "iter 565000, loss: 0.011065\n",
            "iter 566000, loss: 0.011065\n",
            "iter 567000, loss: 0.011065\n",
            "iter 568000, loss: 0.011065\n",
            "iter 569000, loss: 0.011065\n",
            "iter 570000, loss: 0.011064\n",
            "iter 571000, loss: 0.011064\n",
            "iter 572000, loss: 0.011064\n",
            "iter 573000, loss: 0.011064\n",
            "iter 574000, loss: 0.011064\n",
            "iter 575000, loss: 0.011064\n",
            "iter 576000, loss: 0.011063\n",
            "iter 577000, loss: 0.011063\n",
            "iter 578000, loss: 0.011063\n",
            "iter 579000, loss: 0.011063\n",
            "iter 580000, loss: 0.011063\n",
            "iter 581000, loss: 0.011063\n",
            "iter 582000, loss: 0.011062\n",
            "iter 583000, loss: 0.011062\n",
            "iter 584000, loss: 0.011062\n",
            "iter 585000, loss: 0.011062\n",
            "iter 586000, loss: 0.011062\n",
            "iter 587000, loss: 0.011062\n",
            "iter 588000, loss: 0.011061\n",
            "iter 589000, loss: 0.011061\n",
            "iter 590000, loss: 0.011061\n",
            "iter 591000, loss: 0.011061\n",
            "iter 592000, loss: 0.011061\n",
            "iter 593000, loss: 0.011061\n",
            "iter 594000, loss: 0.011061\n",
            "iter 595000, loss: 0.011060\n",
            "iter 596000, loss: 0.011060\n",
            "iter 597000, loss: 0.011060\n",
            "iter 598000, loss: 0.011060\n",
            "iter 599000, loss: 0.011060\n",
            "iter 600000, loss: 0.011060\n",
            "iter 601000, loss: 0.011059\n",
            "iter 602000, loss: 0.011059\n",
            "iter 603000, loss: 0.011059\n",
            "iter 604000, loss: 0.011059\n",
            "iter 605000, loss: 0.011059\n",
            "iter 606000, loss: 0.011059\n",
            "iter 607000, loss: 0.011059\n",
            "iter 608000, loss: 0.011058\n",
            "iter 609000, loss: 0.011058\n",
            "iter 610000, loss: 0.011058\n",
            "iter 611000, loss: 0.011058\n",
            "iter 612000, loss: 0.011058\n",
            "iter 613000, loss: 0.011058\n",
            "iter 614000, loss: 0.011058\n",
            "iter 615000, loss: 0.011057\n",
            "iter 616000, loss: 0.011057\n",
            "iter 617000, loss: 0.011057\n",
            "iter 618000, loss: 0.011057\n",
            "iter 619000, loss: 0.011057\n",
            "iter 620000, loss: 0.011057\n",
            "iter 621000, loss: 0.011057\n",
            "iter 622000, loss: 0.011057\n",
            "iter 623000, loss: 0.011057\n",
            "iter 624000, loss: 0.011058\n",
            "iter 625000, loss: 0.011062\n",
            "iter 626000, loss: 0.011065\n",
            "iter 627000, loss: 0.011068\n",
            "iter 628000, loss: 0.011071\n",
            "iter 629000, loss: 0.011073\n",
            "iter 630000, loss: 0.011075\n",
            "iter 631000, loss: 0.011077\n",
            "iter 632000, loss: 0.011079\n",
            "iter 633000, loss: 0.011081\n",
            "iter 634000, loss: 0.011082\n",
            "iter 635000, loss: 0.011084\n",
            "iter 636000, loss: 0.011085\n",
            "iter 637000, loss: 0.011087\n",
            "iter 638000, loss: 0.011088\n",
            "iter 639000, loss: 0.011089\n",
            "iter 640000, loss: 0.011090\n",
            "iter 641000, loss: 0.011091\n",
            "iter 642000, loss: 0.011092\n",
            "iter 643000, loss: 0.011093\n",
            "iter 644000, loss: 0.011094\n",
            "iter 645000, loss: 0.011095\n",
            "iter 646000, loss: 0.011096\n",
            "iter 647000, loss: 0.011097\n",
            "iter 648000, loss: 0.011097\n",
            "iter 649000, loss: 0.011098\n",
            "iter 650000, loss: 0.011099\n",
            "iter 651000, loss: 0.011099\n",
            "iter 652000, loss: 0.011100\n",
            "iter 653000, loss: 0.011100\n",
            "iter 654000, loss: 0.011101\n",
            "iter 655000, loss: 0.011101\n",
            "iter 656000, loss: 0.011102\n",
            "iter 657000, loss: 0.011102\n",
            "iter 658000, loss: 0.011102\n",
            "iter 659000, loss: 0.011103\n",
            "iter 660000, loss: 0.011103\n",
            "iter 661000, loss: 0.011103\n",
            "iter 662000, loss: 0.011104\n",
            "iter 663000, loss: 0.011104\n",
            "iter 664000, loss: 0.011104\n",
            "iter 665000, loss: 0.011104\n",
            "iter 666000, loss: 0.011105\n",
            "iter 667000, loss: 0.011105\n",
            "iter 668000, loss: 0.011105\n",
            "iter 669000, loss: 0.011105\n",
            "iter 670000, loss: 0.011105\n",
            "iter 671000, loss: 0.011105\n",
            "iter 672000, loss: 0.011106\n",
            "iter 673000, loss: 0.011106\n",
            "iter 674000, loss: 0.011106\n",
            "iter 675000, loss: 0.011106\n",
            "iter 676000, loss: 0.011106\n",
            "iter 677000, loss: 0.011106\n",
            "iter 678000, loss: 0.011106\n",
            "iter 679000, loss: 0.011106\n",
            "iter 680000, loss: 0.011106\n",
            "iter 681000, loss: 0.011106\n",
            "iter 682000, loss: 0.011106\n",
            "iter 683000, loss: 0.011106\n",
            "iter 684000, loss: 0.011106\n",
            "iter 685000, loss: 0.011106\n",
            "iter 686000, loss: 0.011107\n",
            "iter 687000, loss: 0.011107\n",
            "iter 688000, loss: 0.011107\n",
            "iter 689000, loss: 0.011107\n",
            "iter 690000, loss: 0.011107\n",
            "iter 691000, loss: 0.011107\n",
            "iter 692000, loss: 0.011106\n",
            "iter 693000, loss: 0.011106\n",
            "iter 694000, loss: 0.011106\n",
            "iter 695000, loss: 0.011106\n",
            "iter 696000, loss: 0.011107\n",
            "iter 697000, loss: 0.011106\n",
            "iter 698000, loss: 0.011106\n",
            "iter 699000, loss: 0.011106\n",
            "iter 700000, loss: 0.011106\n",
            "iter 701000, loss: 0.011106\n",
            "iter 702000, loss: 0.011106\n",
            "iter 703000, loss: 0.011106\n",
            "iter 704000, loss: 0.011106\n",
            "iter 705000, loss: 0.011106\n",
            "iter 706000, loss: 0.011106\n",
            "iter 707000, loss: 0.011106\n",
            "iter 708000, loss: 0.011106\n",
            "iter 709000, loss: 0.011106\n",
            "iter 710000, loss: 0.011106\n",
            "iter 711000, loss: 0.011106\n",
            "iter 712000, loss: 0.011106\n",
            "iter 713000, loss: 0.011106\n",
            "iter 714000, loss: 0.011105\n",
            "iter 715000, loss: 0.011106\n",
            "iter 716000, loss: 0.011105\n",
            "iter 717000, loss: 0.011105\n",
            "iter 718000, loss: 0.011105\n",
            "iter 719000, loss: 0.011105\n",
            "iter 720000, loss: 0.011105\n",
            "iter 721000, loss: 0.011105\n",
            "iter 722000, loss: 0.011105\n",
            "iter 723000, loss: 0.011105\n",
            "iter 724000, loss: 0.011105\n",
            "iter 725000, loss: 0.011105\n",
            "iter 726000, loss: 0.011105\n",
            "iter 727000, loss: 0.011104\n",
            "iter 728000, loss: 0.011104\n",
            "iter 729000, loss: 0.011104\n",
            "iter 730000, loss: 0.011104\n",
            "iter 731000, loss: 0.011104\n",
            "iter 732000, loss: 0.011104\n",
            "iter 733000, loss: 0.011104\n",
            "iter 734000, loss: 0.011104\n",
            "iter 735000, loss: 0.011104\n",
            "iter 736000, loss: 0.011104\n",
            "iter 737000, loss: 0.011104\n",
            "iter 738000, loss: 0.011103\n",
            "iter 739000, loss: 0.011103\n",
            "iter 740000, loss: 0.011103\n",
            "iter 741000, loss: 0.011103\n",
            "iter 742000, loss: 0.011103\n",
            "iter 743000, loss: 0.011103\n",
            "iter 744000, loss: 0.011103\n",
            "iter 745000, loss: 0.011103\n",
            "iter 746000, loss: 0.011103\n",
            "iter 747000, loss: 0.011103\n",
            "iter 748000, loss: 0.011102\n",
            "iter 749000, loss: 0.011102\n",
            "iter 750000, loss: 0.011103\n",
            "iter 751000, loss: 0.011102\n",
            "iter 752000, loss: 0.011102\n",
            "iter 753000, loss: 0.011102\n",
            "iter 754000, loss: 0.011102\n",
            "iter 755000, loss: 0.011102\n",
            "iter 756000, loss: 0.011102\n",
            "iter 757000, loss: 0.011102\n",
            "iter 758000, loss: 0.011101\n",
            "iter 759000, loss: 0.011101\n",
            "iter 760000, loss: 0.011101\n",
            "iter 761000, loss: 0.011101\n",
            "iter 762000, loss: 0.011101\n",
            "iter 763000, loss: 0.011101\n",
            "iter 764000, loss: 0.011101\n",
            "iter 765000, loss: 0.011101\n",
            "iter 766000, loss: 0.011101\n",
            "iter 767000, loss: 0.011101\n",
            "iter 768000, loss: 0.011101\n",
            "iter 769000, loss: 0.011100\n",
            "iter 770000, loss: 0.011100\n",
            "iter 771000, loss: 0.011100\n",
            "iter 772000, loss: 0.011100\n",
            "iter 773000, loss: 0.011100\n",
            "iter 774000, loss: 0.011100\n",
            "iter 775000, loss: 0.011100\n",
            "iter 776000, loss: 0.011100\n",
            "iter 777000, loss: 0.011100\n",
            "iter 778000, loss: 0.011100\n",
            "iter 779000, loss: 0.011099\n",
            "iter 780000, loss: 0.011099\n",
            "iter 781000, loss: 0.011099\n",
            "iter 782000, loss: 0.011099\n",
            "iter 783000, loss: 0.011099\n",
            "iter 784000, loss: 0.011099\n",
            "iter 785000, loss: 0.011099\n",
            "iter 786000, loss: 0.011099\n",
            "iter 787000, loss: 0.011099\n",
            "iter 788000, loss: 0.011098\n",
            "iter 789000, loss: 0.011098\n",
            "iter 790000, loss: 0.011098\n",
            "iter 791000, loss: 0.011098\n",
            "iter 792000, loss: 0.011098\n",
            "iter 793000, loss: 0.011098\n",
            "iter 794000, loss: 0.011098\n",
            "iter 795000, loss: 0.011098\n",
            "iter 796000, loss: 0.011098\n",
            "iter 797000, loss: 0.011098\n",
            "iter 798000, loss: 0.011098\n",
            "iter 799000, loss: 0.011097\n",
            "iter 800000, loss: 0.011097\n",
            "iter 801000, loss: 0.011097\n",
            "iter 802000, loss: 0.011097\n",
            "iter 803000, loss: 0.011097\n",
            "iter 804000, loss: 0.011097\n",
            "iter 805000, loss: 0.011097\n",
            "iter 806000, loss: 0.011097\n",
            "iter 807000, loss: 0.011097\n",
            "iter 808000, loss: 0.011096\n",
            "iter 809000, loss: 0.011096\n",
            "iter 810000, loss: 0.011096\n",
            "iter 811000, loss: 0.011096\n",
            "iter 812000, loss: 0.011096\n",
            "iter 813000, loss: 0.011096\n",
            "iter 814000, loss: 0.011096\n",
            "iter 815000, loss: 0.011096\n",
            "iter 816000, loss: 0.011096\n",
            "iter 817000, loss: 0.011096\n",
            "iter 818000, loss: 0.011096\n",
            "iter 819000, loss: 0.011095\n",
            "iter 820000, loss: 0.011095\n",
            "iter 821000, loss: 0.011095\n",
            "iter 822000, loss: 0.011095\n",
            "iter 823000, loss: 0.011095\n",
            "iter 824000, loss: 0.011095\n",
            "iter 825000, loss: 0.011095\n",
            "iter 826000, loss: 0.011095\n",
            "iter 827000, loss: 0.011095\n",
            "iter 828000, loss: 0.011095\n",
            "iter 829000, loss: 0.011094\n",
            "iter 830000, loss: 0.011094\n",
            "iter 831000, loss: 0.011094\n",
            "iter 832000, loss: 0.011094\n",
            "iter 833000, loss: 0.011094\n",
            "iter 834000, loss: 0.011094\n",
            "iter 835000, loss: 0.011094\n",
            "iter 836000, loss: 0.011094\n",
            "iter 837000, loss: 0.011093\n",
            "iter 838000, loss: 0.011093\n",
            "iter 839000, loss: 0.011093\n",
            "iter 840000, loss: 0.011093\n",
            "iter 841000, loss: 0.011093\n",
            "iter 842000, loss: 0.011093\n",
            "iter 843000, loss: 0.011093\n",
            "iter 844000, loss: 0.011093\n",
            "iter 845000, loss: 0.011093\n",
            "iter 846000, loss: 0.011093\n",
            "iter 847000, loss: 0.011093\n",
            "iter 848000, loss: 0.011092\n",
            "iter 849000, loss: 0.011092\n",
            "iter 850000, loss: 0.011092\n",
            "iter 851000, loss: 0.011092\n",
            "iter 852000, loss: 0.011092\n",
            "iter 853000, loss: 0.011092\n",
            "iter 854000, loss: 0.011092\n",
            "iter 855000, loss: 0.011092\n",
            "iter 856000, loss: 0.011092\n",
            "iter 857000, loss: 0.011092\n",
            "iter 858000, loss: 0.011091\n",
            "iter 859000, loss: 0.011091\n",
            "iter 860000, loss: 0.011091\n",
            "iter 861000, loss: 0.011091\n",
            "iter 862000, loss: 0.011091\n",
            "iter 863000, loss: 0.011091\n",
            "iter 864000, loss: 0.011091\n",
            "iter 865000, loss: 0.011091\n",
            "iter 866000, loss: 0.011091\n",
            "iter 867000, loss: 0.011091\n",
            "iter 868000, loss: 0.011090\n",
            "iter 869000, loss: 0.011090\n",
            "iter 870000, loss: 0.011090\n",
            "iter 871000, loss: 0.011090\n",
            "iter 872000, loss: 0.011090\n",
            "iter 873000, loss: 0.011090\n",
            "iter 874000, loss: 0.011090\n",
            "iter 875000, loss: 0.011090\n",
            "iter 876000, loss: 0.011090\n",
            "iter 877000, loss: 0.011090\n",
            "iter 878000, loss: 0.011090\n",
            "iter 879000, loss: 0.011089\n",
            "iter 880000, loss: 0.011090\n",
            "iter 881000, loss: 0.011089\n",
            "iter 882000, loss: 0.011089\n",
            "iter 883000, loss: 0.011089\n",
            "iter 884000, loss: 0.011089\n",
            "iter 885000, loss: 0.011089\n",
            "iter 886000, loss: 0.011089\n",
            "iter 887000, loss: 0.011089\n",
            "iter 888000, loss: 0.011089\n",
            "iter 889000, loss: 0.011089\n",
            "iter 890000, loss: 0.011088\n",
            "iter 891000, loss: 0.011088\n",
            "iter 892000, loss: 0.011088\n",
            "iter 893000, loss: 0.011088\n",
            "iter 894000, loss: 0.011088\n",
            "iter 895000, loss: 0.011088\n",
            "iter 896000, loss: 0.011088\n",
            "iter 897000, loss: 0.011088\n",
            "iter 898000, loss: 0.011088\n",
            "iter 899000, loss: 0.011088\n",
            "iter 900000, loss: 0.011088\n",
            "iter 901000, loss: 0.011088\n",
            "iter 902000, loss: 0.011087\n",
            "iter 903000, loss: 0.011087\n",
            "iter 904000, loss: 0.011087\n",
            "iter 905000, loss: 0.011087\n",
            "iter 906000, loss: 0.011087\n",
            "iter 907000, loss: 0.011087\n",
            "iter 908000, loss: 0.011087\n",
            "iter 909000, loss: 0.011087\n",
            "iter 910000, loss: 0.011087\n",
            "iter 911000, loss: 0.011087\n",
            "iter 912000, loss: 0.011086\n",
            "iter 913000, loss: 0.011086\n",
            "iter 914000, loss: 0.011086\n",
            "iter 915000, loss: 0.011086\n",
            "iter 916000, loss: 0.011086\n",
            "iter 917000, loss: 0.011086\n",
            "iter 918000, loss: 0.011086\n",
            "iter 919000, loss: 0.011086\n",
            "iter 920000, loss: 0.011086\n",
            "iter 921000, loss: 0.011086\n",
            "iter 922000, loss: 0.011086\n",
            "iter 923000, loss: 0.011086\n",
            "iter 924000, loss: 0.011085\n",
            "iter 925000, loss: 0.011085\n",
            "iter 926000, loss: 0.011085\n",
            "iter 927000, loss: 0.011085\n",
            "iter 928000, loss: 0.011085\n",
            "iter 929000, loss: 0.011085\n",
            "iter 930000, loss: 0.011085\n",
            "iter 931000, loss: 0.011085\n",
            "iter 932000, loss: 0.011085\n",
            "iter 933000, loss: 0.011085\n",
            "iter 934000, loss: 0.011085\n",
            "iter 935000, loss: 0.011085\n",
            "iter 936000, loss: 0.011084\n",
            "iter 937000, loss: 0.011084\n",
            "iter 938000, loss: 0.011084\n",
            "iter 939000, loss: 0.011084\n",
            "iter 940000, loss: 0.011084\n",
            "iter 941000, loss: 0.011084\n",
            "iter 942000, loss: 0.011084\n",
            "iter 943000, loss: 0.011084\n",
            "iter 944000, loss: 0.011084\n",
            "iter 945000, loss: 0.011084\n",
            "iter 946000, loss: 0.011084\n",
            "iter 947000, loss: 0.011083\n",
            "iter 948000, loss: 0.011083\n",
            "iter 949000, loss: 0.011083\n",
            "iter 950000, loss: 0.011083\n",
            "iter 951000, loss: 0.011083\n",
            "iter 952000, loss: 0.011083\n",
            "iter 953000, loss: 0.011083\n",
            "iter 954000, loss: 0.011083\n",
            "iter 955000, loss: 0.011083\n",
            "iter 956000, loss: 0.011083\n",
            "iter 957000, loss: 0.011083\n",
            "iter 958000, loss: 0.011083\n",
            "iter 959000, loss: 0.011083\n",
            "iter 960000, loss: 0.011082\n",
            "iter 961000, loss: 0.011082\n",
            "iter 962000, loss: 0.011082\n",
            "iter 963000, loss: 0.011082\n",
            "iter 964000, loss: 0.011082\n",
            "iter 965000, loss: 0.011082\n",
            "iter 966000, loss: 0.011082\n",
            "iter 967000, loss: 0.011082\n",
            "iter 968000, loss: 0.011082\n",
            "iter 969000, loss: 0.011082\n",
            "iter 970000, loss: 0.011082\n",
            "iter 971000, loss: 0.011081\n",
            "iter 972000, loss: 0.011081\n",
            "iter 973000, loss: 0.011081\n",
            "iter 974000, loss: 0.011081\n",
            "iter 975000, loss: 0.011081\n",
            "iter 976000, loss: 0.011081\n",
            "iter 977000, loss: 0.011081\n",
            "iter 978000, loss: 0.011081\n",
            "iter 979000, loss: 0.011081\n",
            "iter 980000, loss: 0.011081\n",
            "iter 981000, loss: 0.011081\n",
            "iter 982000, loss: 0.011081\n",
            "iter 983000, loss: 0.011081\n",
            "iter 984000, loss: 0.011080\n",
            "iter 985000, loss: 0.011080\n",
            "iter 986000, loss: 0.011080\n",
            "iter 987000, loss: 0.011080\n",
            "iter 988000, loss: 0.011080\n",
            "iter 989000, loss: 0.011080\n",
            "iter 990000, loss: 0.011080\n",
            "iter 991000, loss: 0.011080\n",
            "iter 992000, loss: 0.011080\n",
            "iter 993000, loss: 0.011080\n",
            "iter 994000, loss: 0.011080\n",
            "iter 995000, loss: 0.011080\n",
            "iter 996000, loss: 0.011079\n",
            "iter 997000, loss: 0.011079\n",
            "iter 998000, loss: 0.011079\n",
            "iter 999000, loss: 0.011079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Z1 = np.dot(W1.T, X) + b1\n",
        "A1 = np.maximum(Z1, 0)\n",
        "Z2 = np.dot(W2.T, A1) + b2\n",
        "predicted_class = np.argmax(Z2, axis=0)\n",
        "print('training accuracy: %.2f %%' % (100*np.mean(predicted_class == y)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VJf5pslbz4f",
        "outputId": "e30b41a4-94f3-476d-a415-39b615d9e4d7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training accuracy: 99.33 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pred(W1, W2, X):\n",
        "    Z1 = np.dot(W1.T, X) + b1\n",
        "    A1 = np.maximum(Z1, 0)\n",
        "    Z2 = np.dot(W2.T, A1) + b2\n",
        "    Y_hat = softmax(Z2)\n",
        "    return np.argmax(Y_hat, axis = 0)\n",
        "\n",
        "xm = np.arange(-2, 11, 0.025)\n",
        "xlen = len(xm)\n",
        "ym = np.arange(-3, 10, 0.025)\n",
        "ylen = len(ym)\n",
        "xx, yy = np.meshgrid(xm, ym)\n",
        "\n",
        "# print(np.ones((1, xx.size)).shape)\n",
        "xx1 = xx.ravel().reshape(1, xx.size)\n",
        "yy1 = yy.ravel().reshape(1, yy.size)\n",
        "\n",
        "# print(xx.shape, yy.shape)\n",
        "XX = np.concatenate((xx1, yy1), axis = 0)\n",
        "# print(XX)\n",
        "# print(X)\n",
        "Z = pred(W1, W2, XX)\n",
        "print(Z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVwOauyKXxA9",
        "outputId": "39d99555-6e66-4b4b-d89f-6233ee25cbb4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 ... 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Put the result into a color plot\n",
        "Z = Z.reshape(xx.shape)\n",
        "CS = plt.contourf(xx, yy, Z, 200, cmap='jet', alpha = .1)\n",
        "\n",
        "# plt.xlabel('Sepal length')\n",
        "# plt.ylabel('Sepal width')\n",
        "\n",
        "plt.xlim(-2, 11)\n",
        "plt.ylim(-3, 10)\n",
        "plt.xticks(())\n",
        "plt.yticks(())\n",
        "plt.axis('equal')\n",
        "\n",
        "plt.plot(X[0, :N], X[1, :N], 'bs', markersize = 7);\n",
        "plt.plot(X[0, N:2*N], X[1, N:2*N], 'ro', markersize = 7);\n",
        "plt.plot(X[0, 2*N:], X[1, 2*N:], 'g^', markersize = 7);\n",
        "# plt.axis('off')\n",
        "plt.xlim([-1.5, 1.5])\n",
        "plt.ylim([-1.5, 1.5])\n",
        "cur_axes = plt.gca()\n",
        "cur_axes.axes.get_xaxis().set_ticks([])\n",
        "cur_axes.axes.get_yaxis().set_ticks([])\n",
        "\n",
        "# plt.savefig('ex1.png', bbox_inches='tight', dpi = 1000)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "SFGtcORRd2IY",
        "outputId": "557a8ae6-17d9-4bd7-c6c7-f18f2351a84b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHzeUPUm84k4um2d9YyGS3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoangndst/ml-notebooks/blob/master/mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XN_8ZYTR-szI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backpropagation\n",
        "- Phương pháp phổ biến nhất để tối ưu MLP vẫn là Gradient Descent (GD). Để áp dụng GD, chúng ta cần tính được gradient của hàm mất mát theo từng ma trận trọng số $\\mathbf{W}^{(l)}$ và và vector bias $\\mathbf{b}^{(l)}$ \n",
        "\n",
        "\\begin{eqnarray}\n",
        "\\mathbf{a}^{(0)} &=& \\mathbf{x} \\newline\n",
        "z_{i}^{(l)} &=& \\mathbf{w}_i^{(l)T}\\mathbf{a}^{(l-1)} + b_i^{(l)} \\newline\n",
        "\\mathbf{z}^{(l)}  &=& \\mathbf{W}^{(l)T}\\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)},~~ l =  1, 2, \\dots, L \\newline\n",
        "\\mathbf{a}^{(l)} &=& f(\\mathbf{z}^{(l)}), ~~ l =  1, 2, \\dots, L \\newline\n",
        "\\mathbf{\\hat{y}} &=& \\mathbf{a}^{(L)}\n",
        "\\end{eqnarray}\n",
        "\n",
        "- Bước này được gói là feedforward vì cách tính toán được thực hiện từ đầu đến cuối của network. MLP cũng được gọi\n",
        "- Giả sử $J(\\mathbf{W, b, X, Y})$ là hàm mất mát.\n",
        "    - Đạo hàm của hàm mất mát theo chỉ một thành phần của ma trận trọng số của lớp cuối cùng:\n",
        "    \\begin{eqnarray}\n",
        "    \\frac{\\partial J}{\\partial w_{ij}^{(L)}} &=& \\frac{\\partial J}{\\partial z_j^{(L)}}. \\frac{\\partial z_j^{(L)}}{\\partial w_{ij}^{(L)}} \\newline\n",
        "    &=& e_j^{(L)} a_i^{(L-1)}\n",
        "    \\end{eqnarray}\n",
        "    - Trong đó $e_j^{(L)} = \\frac{\\partial J}{\\partial z_j^{(L)}}$ là một đại lượng dễ tính. $\\frac{\\partial z_j^{(L)}}{\\partial w_{ij}^{(L)}}  = a_i^{(L-1)}$ là vì $z_j^{(L)} = \\mathbf{w}_j^{(L)T}\\mathbf{a}^{(L-1)} + b_j^{(L)}$\n",
        "    - Tương tự với đạo hàm mất mát theo bias của layer cuối cùng:\n",
        "    \\begin{eqnarray}\n",
        "        \\frac{\\partial J}{\\partial b_{j}^{(L)}} = \\frac{\\partial J}{\\partial z_j^{(L)}}. \\frac{\\partial z_j^{(L)}}{\\partial b_{j}^{(L)}} = e_j^{(L)}\n",
        "    \\end{eqnarray}\n",
        "\n",
        "<img src='https://machinelearningcoban.com/assets/14_mlp/backpropagation.png' width='700px' />\n",
        "\n",
        "- Dựa vào hính trên, ta có thể tính được:\n",
        "    \\begin{eqnarray}\n",
        "\\frac{\\partial J}{\\partial w_{ij}^{(l)}} &=& \\frac{\\partial J}{\\partial z_j^{(l)}}. \\frac{\\partial z_j^{(l)}}{\\partial w_{ij}^{(l)}} \\newline\n",
        "&=& e_j^{(l)} a_i^{(l-1)}\n",
        "\\end{eqnarray}\n",
        "\n",
        "với:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "e_j^{(l)} &=& \\frac{\\partial J}{\\partial z_j^{(l)}} = \\frac{\\partial J}{\\partial a_j^{(l)}} . \\frac{\\partial a_j^{(l)}}{\\partial z_j^{(l)}} \\newline\n",
        "&=& \\left( \\sum_{k = 1}^{d^{(l+1)}} \\frac{\\partial J}{\\partial z_k^{(l+1)}} .\\frac{\\partial z_k^{(l+1)}}{\\partial a_j^{(l)}} \\right) f’(z_j^{(l)}) \\newline\n",
        " &=&\\left( \\sum_{k = 1}^{d^{(l+1)}} e_k^{(l+1)} w_{jk}^{(l+1)} \\right) f’(z_j^{(l)}) \\newline\n",
        " &=&\\left( \\mathbf{w}_{j:}^{(l+1)} \\mathbf{e}^{(l+1)} \\right) f’(z_j^{(l)}) \\newline\n",
        "\\end{eqnarray}\n"
      ],
      "metadata": {
        "id": "J5WLMla8-yOS"
      }
    }
  ]
}